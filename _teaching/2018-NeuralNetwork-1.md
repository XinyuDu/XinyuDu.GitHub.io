---
title: "神经元及神经网络的结构要素"
collection: teaching
type: "神经网络教程-1"
permalink: /teaching/2018-NN-1
venue: "杜新宇,中科院北京纳米能源与系统研究所"
date: 2018-07-13
location: "中国, 北京"
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 1. 神经元

神经元是神经网络的组成元素，任何结构的神经网络模型都是由若干个神经元以某种形式组成的。典型的神经元如图1-1所示。每个神经元可以有若干个输入和输出，一个神经元的输出可以连接到另一个神经元，变成另一个神经元的输入。因此输入和输出个数取决于有多少个神经元与该神经元相连接。

<center>![1-1](2018-NeuralNetwork/1-1.png)</center>

*<center>图1-1 神经元基本结构</center>*

为了简明图1-1只画了一个输出，图中黑色圆圈代表神经元，三条蓝色带箭头的线代表输入神经，红色带箭头的线代表输出神经。$$i_1,i_2,i_3$$为输入值（input），$$w_1,w_2,w_3$$为相对应的权重（weight），$$b$$为偏移量（bias），$$a(z)$$为激活函数（activation），$$o_1$$为输出（output）。整个神经元的运作方式如下式：
$$
o_1=a( w_1\cdot i_1+w_2\cdot i_2+w_3\cdot i_3+b) \tag{1-1}
$$

上式描述了典型的神经元工作方式，输入和权重一一对应相乘后求和，然后再与偏置值求和，求和结果带入激活函数中，所得的函数值即为输出。

# 2. 神经元要素

一个完整的神经元由输入**I**nput，权重**W**eight，偏移量**B**ias，激活函数**A**ctivation和输出**O**utput组成，我称其为**IWBAO**结构。这其中输入**I**和输出**O**很好理解，不再赘述。下面分别介绍**WBA**的作用。

### 2.1 激活函数A

激活函数的作用是将输出值限定在一定范围内，这样做的好处是当改变权重值的时候不会引起输出值的巨大变化。有利于神经网络模型在训练过程中的收敛。常用的激活函数又ReLu，sigmoid和softmax，下面逐一介绍。

**ReLu**（Rectified Linear Unit）激活函数的定义如下：
$$
ReLU(z)=\begin{cases}0\quad if\quad z\leq0\\z\quad if \quad z>0\end{cases}\tag{1-2}
$$
当自变量z的值小于等于0时，函数值为0。当自变量的值大于0时，函数值为z，如图1-2所示。

<center>![1-2](2018-NeuralNetwork/1-2.png)</center>

*<center>图1-2 ReLU函数</center>*

ReLU函数为单边抑制，即z小于等于0时函数值为0。这种特性使得神经网络在学习过程中可以更好的抓住目标特征。此外，ReLU函数的导数是常数，这有利于拟合过程中的稳定收敛。

**Sigmoid**激活函数的定义如下：
$$
\sigma(z)=\frac{1}{1+e^{-z}}\tag{1-3}
$$
当自变量z趋于正无穷时函数趋近于1，当自变量z趋于负无穷时函数趋于0。sigmoid函数将自变量z映射到了[0,1]之间，如图1-3所示。

<center>![1-3](2018-NeuralNetwork/1-3.png)</center>

*<center>图1-3 Sigmoid函数</center>*

Sigmoid函数使得神经元的输出被限定在了[0,1]之间，这使得对权重和偏移量的微小改变导致的输出改变量也是微小的。这样在训练过程中更容易判断调整权重和偏移量的方向和大小。

**Softmax**激活函数的定义如下：
$$
S_i(z_i)=\frac{e^{z_i}}{\Sigma_je^{z_j}}\tag{1-4}
$$
Softmax函数看起来比较晦涩难懂，但其数学意义比较好理解。假设有一组数$$z_i$$，函数值是该自变量$$z_i$$的e指数与所有自变量e指数和的比值。因此，所有$$S_i$$之和为1。该函数可以保证多个神经元输出之和为1，多用于分类任务的神经网络的输出层。比如手写数字识别的神经网络，需要将手写数字分成10个类别，分别对应数字0,1,2,3,...,8,9。softmax函数的值不单单取决于某个神经元的构成元素，还与处于同一层的其他神经元相关，如图1-4所示。

<center>![1-4](2018-NeuralNetwork/1-4.png)</center>

*<center>图1-4 神经元输出层</center>*

图中三个神经元组成了一个神经层，每个神经元的输出不单单和本神经元有关，还与同一层的其他神经元有关，同一层的所有神经元输出之和为1。将上图中的模型参数带入式(1-4)，则各个神经单元的输出可表示为：
$$
o_j=S_i(w_j\cdot i_j+b_j)=\frac{e^{w_j\cdot i_j+b_j}}{\Sigma_k e^{w_k\cdot i_k+b_k}}\\
o_1=S_1=\frac{e^{w_1\cdot i_1+b_1}}{e^{w_1\cdot i_1+b_1}+e^{w_2\cdot i_2+b_2}+e^{w_3\cdot i_3+b_3}}\\
o_2=S_2=\frac{e^{w_2\cdot i_2+b_2}}{e^{w_1\cdot i_1+b_1}+e^{w_2\cdot i_2+b_2}+e^{w_3\cdot i_3+b_3}}\\
o_3=S_3=\frac{e^{w_3\cdot i_3+b_3}}{e^{w_1\cdot i_1+b_1}+e^{w_2\cdot i_2+b_2}+e^{w_3\cdot i_3+b_3}}\tag{1-5}
$$
很容易证明三个输出之和为1，当调整权重和偏移量使得其中一个输出增大时，必然引起另外两个输出的减小，永远保证所有输出量之和为1，这就是softmax的作用。

除了上面详细介绍的ReLU，Sigmoid和Softmax形式的激活函数外还有Tanh，Swish，PReLU和RReLU等多种形式的激活函数，用法大同小异再次不再赘述。

### 2.2 偏移量B

偏移量的作用在于调整激活函数在水平方向上的位置。下面以Sigmoid函数为例说明。如图1-1所示神经元，其输出如式(1-1)所示。令$$z=w_1\cdot i_1+w_2\cdot i_2+w_3\cdot i_3$$，则其输出为：
$$
o_1=\sigma(z+b)=\frac{1}{1+e^{-(z+b)}}\tag{1-6}
$$
我们看看b取不同的值，sigmoid函数会发生那些变化，如图1-5所示。

<center>![1-5](2018-NeuralNetwork/1-5.png)</center>

*<center>图1-5 偏移量对输出的影响</center>*

从图中可以看出，偏移量的变化会引起激活函数沿水平方向平移，这种平移最终影响了输出值的大小。比如当b=0时（图中绿线），z=0对应的函数值为$$\sigma(0)=0.5$$，当b=-2时（图中蓝色线），z=0对应的函数值约等于0.88。

### 2.3 权重W

权重显而易见的决定着输出值的大小，如图1-5所示，权重的大小决定着函数在那个点取值，即权重决定了z的大小。除此之外权重还代表着神经元的各个输入的重要性，即某个输入影响输出能力的大小。神经网络训练的过程就是不断调整权重和偏移量的过程，最终找到一组权重和偏移量使得模型的输出和我们期待的输出之间的差值最小。